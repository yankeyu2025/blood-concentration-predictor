#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import pandas as pd
import numpy as np
import pickle
import json
import os
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def retrain_model_for_low_concentration():
    """é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œä¸“é—¨è¯†åˆ«è¡€è¯æµ“åº¦æ˜¯å¦å°äº0.5"""
    
    print("ğŸ”„ å¼€å§‹é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè¯†åˆ«è¡€è¯æµ“åº¦ < 0.5...")
    
    # 1. åŠ è½½è®­ç»ƒæ•°æ®
    try:
        # æ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„æŸ¥æ‰¾è®­ç»ƒæ•°æ®
        possible_paths = [
            'train_set_concentration.csv',
            '../train_set_concentration.csv',
            os.path.join('..', 'train_set_concentration.csv'),
            os.path.join(os.path.dirname(os.path.dirname(__file__)), 'train_set_concentration.csv')
        ]
        
        # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼
        encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb18030', 'cp936', 'latin-1']
        train_data = None
        
        for path in possible_paths:
            if os.path.exists(path):
                for encoding in encodings:
                    try:
                        train_data = pd.read_csv(path, encoding=encoding)
                        print(f"âœ… è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ (è·¯å¾„: {path}, ç¼–ç : {encoding})ï¼Œå…± {len(train_data)} æ¡è®°å½•")
                        break
                    except UnicodeDecodeError:
                        continue
                if train_data is not None:
                    break
        
        if train_data is None:
            print("âŒ æ— æ³•æ‰¾åˆ°æˆ–è¯»å–è®­ç»ƒæ•°æ®æ–‡ä»¶")
            print("å°è¯•çš„è·¯å¾„:")
            for path in possible_paths:
                print(f"  - {path} (å­˜åœ¨: {os.path.exists(path)})")
            return
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # 2. æ•°æ®é¢„å¤„ç†
    # é€‰æ‹©ç‰¹å¾ï¼ˆä¸å½“å‰Webåº”ç”¨ä¸€è‡´ï¼‰
    feature_columns = [
        'Daily doseï¼ˆgï¼‰',
        'CLCR', 
        'GGT(U/L)',
        'Na(mmol/L)',
        'HDL-C(mmol/L)',
        'ALB(g/L)'
    ]
    
    # ç›®æ ‡åˆ—
    target_column = 'concentrationï¼ˆng/mlï¼‰'
    
    # æ£€æŸ¥æ•°æ®
    print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"ç‰¹å¾åˆ—: {feature_columns}")
    print(f"ç›®æ ‡åˆ—: {target_column}")
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_features = []
    for col in feature_columns:
        if col not in train_data.columns:
            missing_features.append(col)
    
    if missing_features:
        print(f"âŒ ç¼ºå¤±ç‰¹å¾åˆ—: {missing_features}")
        print("å¯ç”¨åˆ—:", list(train_data.columns))
        return
    
    if target_column not in train_data.columns:
        print(f"âŒ ç¼ºå¤±ç›®æ ‡åˆ—: {target_column}")
        print("å¯ç”¨åˆ—:", list(train_data.columns))
        return
    
    # æå–ç‰¹å¾å’Œç›®æ ‡
    X = train_data[feature_columns].copy()
    y_continuous = train_data[target_column].copy()
    
    print(f"\nğŸ“ˆ è¡€è¯æµ“åº¦åˆ†å¸ƒç»Ÿè®¡:")
    print(f"æœ€å°å€¼: {y_continuous.min():.3f}")
    print(f"æœ€å¤§å€¼: {y_continuous.max():.3f}")
    print(f"å¹³å‡å€¼: {y_continuous.mean():.3f}")
    print(f"ä¸­ä½æ•°: {y_continuous.median():.3f}")
    
    # 3. é‡æ–°å®šä¹‰æ ‡ç­¾
    # æ–°çš„æ ‡ç­¾å®šä¹‰ï¼š0(æ­£å¸¸) = è¡€è¯æµ“åº¦ >= 0.5, 1(å¼‚å¸¸) = è¡€è¯æµ“åº¦ < 0.5
    y_binary = (y_continuous < 0.5).astype(int)
    
    print(f"\nğŸ·ï¸  æ–°æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"å¼‚å¸¸ (< 0.5): {sum(y_binary == 1)} æ¡ ({sum(y_binary == 1)/len(y_binary)*100:.1f}%)")
    print(f"æ­£å¸¸ (â‰¥ 0.5): {sum(y_binary == 0)} æ¡ ({sum(y_binary == 0)/len(y_binary)*100:.1f}%)")
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    print(f"\nğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:")
    print(f"ç‰¹å¾ç¼ºå¤±å€¼: {X.isnull().sum().sum()}")
    print(f"ç›®æ ‡ç¼ºå¤±å€¼: {y_binary.isnull().sum()}")
    
    # åˆ é™¤ç¼ºå¤±å€¼
    mask = ~(X.isnull().any(axis=1) | y_binary.isnull())
    X_clean = X[mask]
    y_clean = y_binary[mask]
    
    print(f"æ¸…ç†åæ•°æ®: {len(X_clean)} æ¡è®°å½•")
    
    # 4. æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean
    )
    
    print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
    print(f"è®­ç»ƒé›†: {len(X_train)} æ¡")
    print(f"æµ‹è¯•é›†: {len(X_test)} æ¡")
    
    # 5. ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print("âœ… ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆ")
    
    # 6. è®­ç»ƒæ¨¡å‹
    print(f"\nğŸ¤– å¼€å§‹è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
    
    model = LogisticRegression(
        max_iter=1000,
        solver='liblinear',
        random_state=42,
        class_weight='balanced'  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    )
    
    model.fit(X_train_scaled, y_train)
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    # 7. æ¨¡å‹è¯„ä¼°
    y_pred = model.predict(X_test_scaled)
    y_prob = model.predict_proba(X_test_scaled)[:, 1]
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°:")
    print(f"å‡†ç¡®ç‡: {accuracy:.3f}")
    print(f"ç²¾ç¡®ç‡: {precision:.3f}")
    print(f"å¬å›ç‡: {recall:.3f}")
    print(f"F1åˆ†æ•°: {f1:.3f}")
    print(f"AUCå€¼: {auc:.3f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    print(f"\næ··æ·†çŸ©é˜µ:")
    print(f"çœŸå®æ­£å¸¸/é¢„æµ‹æ­£å¸¸: {cm[0,0]}")
    print(f"çœŸå®æ­£å¸¸/é¢„æµ‹å¼‚å¸¸: {cm[0,1]}")
    print(f"çœŸå®å¼‚å¸¸/é¢„æµ‹æ­£å¸¸: {cm[1,0]}")
    print(f"çœŸå®å¼‚å¸¸/é¢„æµ‹å¼‚å¸¸: {cm[1,1]}")
    
    # 8. ä¿å­˜æ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æ–°æ¨¡å‹...")
    
    # ä¿å­˜æ¨¡å‹
    with open('web_models/logistic_regression_model.pkl', 'wb') as f:
        pickle.dump(model, f)
    
    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    with open('web_models/feature_scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    # æ›´æ–°å…ƒæ•°æ®
    metadata = {
        "model_type": "LogisticRegression",
        "features": feature_columns,
        "feature_count": len(feature_columns),
        "thresholds": {
            "binary": 0.5
        },
        "label_definition": {
            "0": "æ­£å¸¸ (è¡€è¯æµ“åº¦ â‰¥ 0.5)",
            "1": "å¼‚å¸¸ (è¡€è¯æµ“åº¦ < 0.5)"
        },
        "feature_descriptions": {
            "Daily doseï¼ˆgï¼‰": "æ—¥å‰‚é‡ï¼ˆå…‹ï¼‰",
            "CLCR": "è‚Œé…æ¸…é™¤ç‡",
            "GGT(U/L)": "Î³-è°·æ°¨é…°è½¬ç§»é…¶",
            "Na(mmol/L)": "é’ ç¦»å­æµ“åº¦",
            "HDL-C(mmol/L)": "é«˜å¯†åº¦è„‚è›‹ç™½èƒ†å›ºé†‡",
            "ALB(g/L)": "ç™½è›‹ç™½"
        },
        "clinical_ranges": {
            "Daily doseï¼ˆgï¼‰": {"min": 0.1, "max": 2.0, "unit": "g"},
            "CLCR": {"min": 30, "max": 150, "unit": "mL/min"},
            "GGT(U/L)": {"min": 5, "max": 200, "unit": "U/L"},
            "Na(mmol/L)": {"min": 135, "max": 145, "unit": "mmol/L"},
            "HDL-C(mmol/L)": {"min": 0.8, "max": 2.5, "unit": "mmol/L"},
            "ALB(g/L)": {"min": 35, "max": 55, "unit": "g/L"}
        },
        "performance": {
            "accuracy": float(accuracy),
            "precision": float(precision),
            "recall": float(recall),
            "f1_score": float(f1),
            "auc": float(auc)
        },
        "training_info": {
            "training_samples": len(X_train),
            "test_samples": len(X_test),
            "class_distribution": {
                "normal": int(sum(y_clean == 0)),
                "abnormal": int(sum(y_clean == 1))
            }
        }
    }
    
    with open('web_models/model_metadata.json', 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print("âœ… æ¨¡å‹å’Œå…ƒæ•°æ®ä¿å­˜å®Œæˆ")
    
    # 9. æµ‹è¯•æ–°æ¨¡å‹
    print(f"\nğŸ§ª æµ‹è¯•æ–°æ¨¡å‹...")
    
    test_cases = [
        {
            'name': 'æä½è¡€è¯æµ“åº¦æµ‹è¯•',
            'data': [0.1, 120.0, 15.0, 142.0, 2.0, 50.0],
            'expected': 'å¼‚å¸¸ (< 0.5)'
        },
        {
            'name': 'æ­£å¸¸è¡€è¯æµ“åº¦æµ‹è¯•',
            'data': [0.8, 80.0, 25.0, 140.0, 1.2, 42.0],
            'expected': 'æ­£å¸¸ (â‰¥ 0.5)'
        },
        {
            'name': 'é«˜å‰‚é‡æµ‹è¯•',
            'data': [2.0, 40.0, 60.0, 135.0, 0.8, 35.0],
            'expected': 'æ­£å¸¸ (â‰¥ 0.5)'
        }
    ]
    
    for test_case in test_cases:
        test_input = np.array([test_case['data']])
        test_scaled = scaler.transform(test_input)
        
        prediction = model.predict(test_scaled)[0]
        probability = model.predict_proba(test_scaled)[0]
        
        result = "å¼‚å¸¸ (< 0.5)" if prediction == 1 else "æ­£å¸¸ (â‰¥ 0.5)"
        
        print(f"\n{test_case['name']}:")
        print(f"  è¾“å…¥: {test_case['data']}")
        print(f"  é¢„æµ‹: {result}")
        print(f"  æœŸæœ›: {test_case['expected']}")
        print(f"  æ¦‚ç‡: [æ­£å¸¸: {probability[0]:.3f}, å¼‚å¸¸: {probability[1]:.3f}]")
        print(f"  âœ… æ­£ç¡®" if result == test_case['expected'] else f"  âŒ é”™è¯¯")
    
    print(f"\nğŸ‰ æ¨¡å‹é‡æ–°è®­ç»ƒå®Œæˆï¼")
    print(f"æ–°æ¨¡å‹ä¸“é—¨è¯†åˆ«è¡€è¯æµ“åº¦ < 0.5 ä¸ºå¼‚å¸¸")
    print(f"æ¨¡å‹æ€§èƒ½: å‡†ç¡®ç‡ {accuracy:.3f}, AUC {auc:.3f}")

if __name__ == "__main__":
    retrain_model_for_low_concentration()