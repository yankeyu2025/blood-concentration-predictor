# -*- coding: utf-8 -*-
"""
ä¿å­˜æœ€ä¼˜æ¨¡å‹å’Œç‰¹å¾å¤„ç†å™¨
ç”¨äºWebåº”ç”¨éƒ¨ç½²
"""
import os, json, pickle, warnings
warnings.filterwarnings('ignore')
import numpy as np, pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, roc_auc_score
from imblearn.over_sampling import SMOTE

BASE_DIR = os.path.abspath(os.path.dirname(__file__))
CONFIG_PATH = os.path.join(BASE_DIR, 'æµç¨‹é…ç½®.json')
DATA_FILES = {
    'train': os.path.join(os.path.dirname(BASE_DIR), 'train_set_concentration.csv'),
    'test': os.path.join(os.path.dirname(BASE_DIR), 'test_set_concentration.csv')
}
TARGET_NAME = 'concentration(ng/ml)'
RESULT_ROOT = os.path.join(BASE_DIR, 'ç»“æœ')
MODEL_DIR = os.path.join(BASE_DIR, 'web_models')

ENCODINGS = ['utf-8','utf-8-sig','gbk','gb18030','cp936']

def read_csv_any(path):
    for enc in ENCODINGS:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            pass
    return pd.read_csv(path)

def normalize_target(df):
    t = None
    for c in df.columns:
        if 'concentration' in c.lower(): 
            t = c
            break
    if t and t != TARGET_NAME: 
        df = df.rename(columns={t: TARGET_NAME})
    return df

def load_config():
    cfg = {
        'include_clcr': True,
        'drop_clcr_sources': True,
        'clcr_source_columns': ['UREA','weight','age'],
        'thresholds': {'binary':0.5, 'three_class_low':0.5, 'three_class_high':1.2},
        'use_standard_scaler': True,
        'feature_selection': {'pearson_min_abs_corr':0.1, 'use_boruta':True}
    }
    if os.path.isfile(CONFIG_PATH):
        try:
            user = json.load(open(CONFIG_PATH,'r',encoding='utf-8'))
            cfg.update(user)
        except Exception:
            pass
    return cfg

def build_binary_labels(df, cfg):
    """æ„å»ºäºŒåˆ†ç±»æ ‡ç­¾"""
    y = df[TARGET_NAME].astype(float)
    th = cfg['thresholds']['binary']
    return (y >= th).astype(int).values

def match_any(col, keys):
    lc = col.lower()
    for k in keys:
        lk = k.lower()
        if lk in lc:
            return True
    cn_map = {'urea':'å°¿ç´ ','weight':'ä½“é‡','age':'å¹´é¾„'}
    for en, cn in cn_map.items():
        if en in keys and cn in lc:
            return True
    return False

def load_final_features():
    """åŠ è½½æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾"""
    feature_file = os.path.join(RESULT_ROOT, 'binary_æœªä½¿ç”¨SMOTE', 'æœ€ç»ˆç‰¹å¾_binary_æœªä½¿ç”¨SMOTE.json')
    with open(feature_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data['æœ€ç»ˆç‰¹å¾']

def preprocess_data(cfg):
    """æ•°æ®é¢„å¤„ç†"""
    print("ğŸ“Š åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    
    # åŠ è½½æ•°æ®
    train_df = normalize_target(read_csv_any(DATA_FILES['train']))
    test_df = normalize_target(read_csv_any(DATA_FILES['test']))
    
    # åˆ é™¤CLCRç›¸å…³åˆ—
    if cfg['drop_clcr_sources']:
        drop_cols = [c for c in train_df.columns if match_any(c, cfg['clcr_source_columns'])]
        train_df = train_df.drop(columns=drop_cols, errors='ignore')
        test_df = test_df.drop(columns=drop_cols, errors='ignore')
    
    # åŠ è½½æœ€ç»ˆç‰¹å¾
    final_features = load_final_features()
    print(f"ğŸ“‹ ä½¿ç”¨ç‰¹å¾: {final_features}")
    
    # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
    X_train = train_df[final_features].copy()
    X_test = test_df[final_features].copy()
    
    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼å‹
    for col in final_features:
        X_train[col] = pd.to_numeric(X_train[col], errors='coerce')
        X_test[col] = pd.to_numeric(X_test[col], errors='coerce')
    
    # å¡«å……ç¼ºå¤±å€¼
    X_train = X_train.fillna(X_train.mean())
    X_test = X_test.fillna(X_train.mean())
    
    # æ£€æŸ¥æ•°æ®åˆ†å¸ƒå¹¶å¤„ç†å¼‚å¸¸å€¼
    print("ğŸ” æ£€æŸ¥æ•°æ®åˆ†å¸ƒ...")
    for col in final_features:
        q1 = X_train[col].quantile(0.25)
        q3 = X_train[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        
        # å¤„ç†å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨è¾¹ç•Œå€¼æ›¿æ¢ï¼‰
        outliers_count = ((X_train[col] < lower_bound) | (X_train[col] > upper_bound)).sum()
        if outliers_count > 0:
            print(f"   {col}: å‘ç° {outliers_count} ä¸ªå¼‚å¸¸å€¼ï¼Œè¿›è¡Œå¤„ç†")
            X_train[col] = X_train[col].clip(lower_bound, upper_bound)
            X_test[col] = X_test[col].clip(lower_bound, upper_bound)
    
    y_train = build_binary_labels(train_df, cfg)
    y_test = build_binary_labels(test_df, cfg)
    
    print(f"âœ… è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
    print(f"ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ - è®­ç»ƒé›†: {np.bincount(y_train)}, æµ‹è¯•é›†: {np.bincount(y_test)}")
    
    # æ‰“å°ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯:")
    for col in final_features:
        print(f"   {col}: mean={X_train[col].mean():.3f}, std={X_train[col].std():.3f}")
    
    return X_train, X_test, y_train, y_test, final_features

def train_final_model(X_train, y_train, final_features):
    """è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
    print("ğŸ”§ è®­ç»ƒæœ€ç»ˆçš„é€»è¾‘å›å½’æ¨¡å‹...")
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_train)
    
    # ä½¿ç”¨SMOTEå¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    print("âš–ï¸ ä½¿ç”¨SMOTEå¤„ç†ç±»åˆ«ä¸å¹³è¡¡...")
    smote = SMOTE(random_state=42)
    X_balanced, y_balanced = smote.fit_resample(X_scaled, y_train)
    
    print(f"ğŸ“ˆ SMOTEåç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_balanced)}")
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„å‚æ•°ï¼‰
    model = LogisticRegression(
        C=1,  # ä½¿ç”¨è¶…å‚æ•°ä¼˜åŒ–å¾—åˆ°çš„æœ€ä½³å‚æ•°
        max_iter=1000,
        solver='liblinear',
        random_state=42,
        class_weight='balanced'  # æ·»åŠ ç±»åˆ«æƒé‡å¹³è¡¡
    )
    model.fit(X_balanced, y_balanced)
    
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    return model, scaler

def evaluate_final_model(model, scaler, X_test, y_test):
    """è¯„ä¼°æœ€ç»ˆæ¨¡å‹"""
    print("ğŸ“Š è¯„ä¼°æœ€ç»ˆæ¨¡å‹...")
    
    X_test_scaled = scaler.transform(X_test)
    y_pred = model.predict(X_test_scaled)
    y_prob = model.predict_proba(X_test_scaled)[:, 1]
    
    accuracy = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    
    print(f"ğŸ¯ æœ€ç»ˆæ¨¡å‹æ€§èƒ½:")
    print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"   AUC: {auc:.4f}")
    
    return {'accuracy': accuracy, 'auc': auc}

def save_model_for_web(model, scaler, final_features, performance, cfg):
    """ä¿å­˜æ¨¡å‹ç”¨äºWebåº”ç”¨"""
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹ç”¨äºWebåº”ç”¨...")
    
    os.makedirs(MODEL_DIR, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    with open(os.path.join(MODEL_DIR, 'logistic_regression_model.pkl'), 'wb') as f:
        pickle.dump(model, f)
    
    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    with open(os.path.join(MODEL_DIR, 'feature_scaler.pkl'), 'wb') as f:
        pickle.dump(scaler, f)
    
    # ä¿å­˜æ¨¡å‹å…ƒæ•°æ®
    metadata = {
        'model_type': 'LogisticRegression',
        'features': final_features,
        'feature_count': len(final_features),
        'performance': performance,
        'thresholds': cfg['thresholds'],
        'model_params': model.get_params(),
        'feature_descriptions': {
            'Daily doseï¼ˆgï¼‰': 'æ—¥å‰‚é‡ï¼ˆå…‹ï¼‰',
            'CLCR': 'è‚Œé…æ¸…é™¤ç‡',
            'GGT(U/L)': 'Î³-è°·æ°¨é…°è½¬ç§»é…¶',
            'Na(mmol/L)': 'é’ ç¦»å­æµ“åº¦',
            'HDL-C(mmol/L)': 'é«˜å¯†åº¦è„‚è›‹ç™½èƒ†å›ºé†‡',
            'ALB(g/L)': 'ç™½è›‹ç™½'
        },
        'clinical_ranges': {
            'Daily doseï¼ˆgï¼‰': {'min': 0.1, 'max': 2.0, 'unit': 'g'},
            'CLCR': {'min': 30, 'max': 150, 'unit': 'mL/min'},
            'GGT(U/L)': {'min': 5, 'max': 200, 'unit': 'U/L'},
            'Na(mmol/L)': {'min': 135, 'max': 145, 'unit': 'mmol/L'},
            'HDL-C(mmol/L)': {'min': 0.8, 'max': 2.5, 'unit': 'mmol/L'},
            'ALB(g/L)': {'min': 35, 'max': 55, 'unit': 'g/L'}
        }
    }
    
    with open(os.path.join(MODEL_DIR, 'model_metadata.json'), 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    # åˆ›å»ºé¢„æµ‹å‡½æ•°ç¤ºä¾‹
    prediction_example = '''
# ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹çš„ç¤ºä¾‹ä»£ç 
import pickle
import numpy as np
import pandas as pd

# åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
with open('logistic_regression_model.pkl', 'rb') as f:
    model = pickle.load(f)

with open('feature_scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)

# ç¤ºä¾‹è¾“å…¥æ•°æ®
sample_data = {
    'Daily doseï¼ˆgï¼‰': 0.8,
    'CLCR': 80.0,
    'GGT(U/L)': 25.0,
    'Na(mmol/L)': 140.0,
    'HDL-C(mmol/L)': 1.2,
    'ALB(g/L)': 42.0
}

# è½¬æ¢ä¸ºDataFrameå¹¶æ ‡å‡†åŒ–
X = pd.DataFrame([sample_data])
X_scaled = scaler.transform(X)

# é¢„æµ‹
probability = model.predict_proba(X_scaled)[0, 1]
prediction = model.predict(X_scaled)[0]

print(f"è¡€è¯æµ“åº¦åé«˜æ¦‚ç‡: {probability:.3f}")
print(f"é¢„æµ‹ç»“æœ: {'åé«˜' if prediction == 1 else 'æ­£å¸¸'}")
'''
    
    with open(os.path.join(MODEL_DIR, 'prediction_example.py'), 'w', encoding='utf-8') as f:
        f.write(prediction_example)
    
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_DIR}")
    print(f"ğŸ“ åŒ…å«æ–‡ä»¶:")
    print(f"   - logistic_regression_model.pkl (æ¨¡å‹æ–‡ä»¶)")
    print(f"   - feature_scaler.pkl (æ ‡å‡†åŒ–å™¨)")
    print(f"   - model_metadata.json (æ¨¡å‹å…ƒæ•°æ®)")
    print(f"   - prediction_example.py (ä½¿ç”¨ç¤ºä¾‹)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä¿å­˜æœ€ä¼˜æ¨¡å‹...")
    
    # åŠ è½½é…ç½®
    cfg = load_config()
    
    # æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test, final_features = preprocess_data(cfg)
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    model, scaler = train_final_model(X_train, y_train, final_features)
    
    # è¯„ä¼°æ¨¡å‹
    performance = evaluate_final_model(model, scaler, X_test, y_test)
    
    # ä¿å­˜æ¨¡å‹
    save_model_for_web(model, scaler, final_features, performance, cfg)
    
    print("ğŸ‰ æ¨¡å‹ä¿å­˜å®Œæˆï¼å¯ä»¥å¼€å§‹åˆ›å»ºWebåº”ç”¨äº†ã€‚")
    
    return model, scaler, final_features, performance

if __name__ == '__main__':
    main()